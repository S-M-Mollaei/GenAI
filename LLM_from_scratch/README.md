# GenAI

Fromâ€‘scratch **GPT-style language model** in PyTorch with a minimal tokenizer, attention, and Transformer blocks â€” plus **fineâ€‘tuning** for **SMS spam classification** and **instruction tuning**. This repo is centered around the notebook `hands_on_llm_fine_tuned.ipynb`.

## âœ¨ Highlights
- **Custom tokenizer** (basic + Byteâ€‘Pair Encoding) and tokenâ†’ID pipeline
- **Embeddings + positional encodings**
- **Selfâ€‘Attention / Causal Attention / Multiâ€‘Head Attention**
- **Transformer blocks** (LayerNorm, GELU, residual/shortcut connections)
- **GPTâ€‘style model** with text generation
- **Training loop** with train/val loss evaluation & weight saving/loading
- **Fineâ€‘tuning for classification** on the UCI SMS Spam Collection dataset
- **Instruction fineâ€‘tuning** with a small JSON instruction dataset
- Utility cells for **reproducibility** (seeds) and **data loaders**

> Built to be educational and hackable â€” great as a learning scaffold or a base for small experiments.

---

## ğŸ“ Project Structure
```
hands_on_llm_fine_tuned.ipynb   # Main notebook with full pipeline
data/                           # (Optional) place datasets here
â”œâ”€ sms_spam_collection.zip      # UCI SMS Spam Collection (or extracted CSVs)
â”œâ”€ instruction-data.json        # Instruction tuning data
â””â”€ the-verdict.txt              # Sample text for pretraining demos
```

Datasets are downloaded inâ€‘notebook from:
- UCI SMS Spam: `https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip`
- Instruction data (example path used in notebook): `instruction-data.json`
- Sample text: `the-verdict.txt` (from the LLMsâ€‘fromâ€‘scratch examples)

---

## ğŸ›  Requirements
- Python 3.9+
- PyTorch
- NumPy, pandas, matplotlib, tqdm
- (Optional) `tiktoken`, `gensim`, `tensorflow` (used in some cells)
- Jupyter

You can capture exact versions with:
```bash
pip freeze > requirements.txt
```

---

## ğŸš€ Quickstart
1. **Create and activate a virtual environment**
```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```

2. **Install packages**
```bash
pip install torch numpy pandas matplotlib tqdm tiktoken gensim tensorflow jupyter
```

3. **Run the notebook**
```bash
jupyter notebook hands_on_llm_fine_tuned.ipynb
```
Follow the sections in order (tokenizer â†’ attention â†’ GPT â†’ training â†’ fineâ€‘tuning).

---

## ğŸ§ª Fineâ€‘Tuning for SMS Spam Classification
The notebook defines a `SpamDataset` and fineâ€‘tunes the model for a binary classifier.

**Data**  
- Source: UCI SMS Spam Collection  
- Files (after extraction): `train.csv`, `validation.csv`, `test.csv` (or adapt paths used in the notebook)

**Run**  
Execute the section **â€œFineâ€‘Tune for Classificationâ€**:
- Preprocess & tokenize
- Create `DataLoader`s
- Initialize model head for classification
- Train and track **loss & accuracy**
- Evaluate on test set

---

## ğŸ“ Instruction Fineâ€‘Tuning
A lightweight **`InstructionDataset`** is used to fineâ€‘tune the base GPT for instructionâ€“response behavior.

**Run**  
Execute the section **â€œInstruction Fineâ€‘Tuningâ€**:
- Load `instruction-data.json` (prompt/response pairs)
- Build training batches & `DataLoader`
- Load the pretrained (base) model
- Fineâ€‘tune and sample generations

---

## ğŸ’¾ Saving / Loading Weights
The notebook includes utilities to:
- `torch.save(state_dict)` after training
- `model.load_state_dict(...)` to resume

Use the **â€œLoading and Saving Model Weightsâ€** section to persist experiments.

---

## ğŸ“Š Evaluation & Generation
- Evaluate **train/val loss** with helper functions
- Generate text from random or trained weights (sampling topâ€‘k/temperature depending on your edits)

---

## âš ï¸ Notes
- The code is designed for **clarity over speed**; expect educational readability.
- Some sections use optional libraries (`tiktoken`, `tensorflow`, `gensim`). If not needed, you can skip those cells.
- GPU recommended for faster training (configure your PyTorch device).

---

## ğŸ§¾ License
MIT (see `LICENSE` file).

## ğŸ™ Acknowledgments
Parts of the data paths and sample text are inspired by the â€œLLMs from Scratchâ€ educational materials.
